{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import sys\n",
    "sys.path.append(f'../src')\n",
    "from importlib import reload \n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.optim import Adam\n",
    "import gym\n",
    "import time\n",
    "\n",
    "import sac_tri\n",
    "import sac_tri_envs\n",
    "import core_tri\n",
    "\n",
    "reload(sac_tri);\n",
    "reload(sac_tri_envs);\n",
    "reload(core_tri);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train a two-level system in the thermalization dominated regime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a_val = 0.7\n",
    "\n",
    "env_params = {\n",
    "    \"g\": 1.,                      \n",
    "    \"b\": 1.,\n",
    "    \"min_u\": -0.8,\n",
    "    \"max_u\": 0.8,\n",
    "    \"e0\": 5.,                     \n",
    "    \"dt\": 0.07,   \n",
    "    \"a\": a_val,\n",
    "    \"pow_coeff\": 1.,\n",
    "    \"diss_coeff\": 1.,\n",
    "}  \n",
    "training_hyperparams = {\n",
    "    \"BATCH_SIZE\": 256,              #batch size\n",
    "    \"LR\": 0.001,                    #learning rate\n",
    "    \"ALPHA_LR\": 0.003,\n",
    "    \"H_D_START\": np.log(3.),        #the exploration coeff\n",
    "    \"H_D_END\": 0.01,                #the exploration coeff\n",
    "    \"H_D_DECAY\": 30000,            #the exploration coeff, in \"units\" of steps\n",
    "    \"H_C_START\": 0.8,              #the exploration coeff\n",
    "    \"H_C_END\": -3.,                #3.5 INSTEAD OF -7 SINCE HERE 1 ACTION, WHILE -7 2 ACTIONS\n",
    "    \"H_C_DECAY\": 60000,           #the exploration coeff, in \"units\" of steps\n",
    "    \"REPLAY_MEMORY_SIZE\": 80000,  #IT WAS 2000\n",
    "    \"POLYAK\": 0.995,                #polyak coefficient\n",
    "    \"LOG_STEPS\": 1000,              #save logs and display training every number of steps\n",
    "    \"GAMMA\": 0.998,                 #RL discount factor\n",
    "    \"HIDDEN_SIZES\": (256,128),      #size of hidden layers \n",
    "    \"SAVE_STATE_STEPS\": 160000,      #saves complete state of trainig every number of steps\n",
    "    \"INITIAL_RANDOM_STEPS\": 5000,   #number of initial uniformly random steps\n",
    "    \"UPDATE_AFTER\": 1000,           #start minimizing loss function after initial steps\n",
    "    \"UPDATE_EVERY\": 50,             #performs this many updates every this many steps\n",
    "    \"USE_CUDA\": False,               #use cuda for computation\n",
    "    \"MIN_COV_EIGEN\": 1.e-8,\n",
    "    \"DONT_SAVE_MEMORY\": False\n",
    "}\n",
    "log_info = {\n",
    "    \"log_running_reward\": True,     #log running reward \n",
    "    \"log_running_loss\": True,       #log running loss\n",
    "    \"log_actions\": True,            #log chosen actions\n",
    "    \"extra_str\": f\"_a={a_val}_newenv\" #extra string to append to training folder\n",
    "}\n",
    "\n",
    "train = sac_tri.SacTrain()\n",
    "train.initialize_new_train(sac_tri_envs.TwoLevelBosonicFeedbackDemonPowDiss, env_params, training_hyperparams, log_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.train(160000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate the current policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.evaluate_current_policy(deterministic=True, steps=10000,\n",
    "                              actions_to_plot=200, gamma=0.9999)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.save_full_state()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_dir = \"../data/2022_07_05-14_32_42_a=0.95\"\n",
    "\n",
    "train = sac_tri.SacTrain()\n",
    "train.load_train(state_dir,no_train=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Continue training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.train(50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## view training status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.plot_logs()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
